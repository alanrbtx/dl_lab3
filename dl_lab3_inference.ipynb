{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import os\n",
    "import pathlib\n",
    "import cv2\n",
    "import time\n",
    "import argparse\n",
    "import requests\n",
    "from transformers import pipeline, WhisperTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VIDEO BLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video processing: 0.0\n",
      "Video processing: 1.0\n",
      "Video processing: 2.0\n",
      "Video processing: 3.0\n",
      "Video processing: 4.0\n",
      "Video processing: 5.0\n",
      "Video processing: 6.0\n",
      "Video processing: 7.0\n",
      "Video processing: 8.0\n",
      "Processing by BLIP: video_frames/frame$$0.0$$.jpg\n",
      "Processing by BLIP: video_frames/frame$$1.0$$.jpg\n",
      "Processing by BLIP: video_frames/frame$$2.0$$.jpg\n",
      "Processing by BLIP: video_frames/frame$$3.0$$.jpg\n",
      "Processing by BLIP: video_frames/frame$$4.0$$.jpg\n",
      "Processing by BLIP: video_frames/frame$$5.0$$.jpg\n",
      "Processing by BLIP: video_frames/frame$$6.0$$.jpg\n",
      "Processing by BLIP: video_frames/frame$$7.0$$.jpg\n",
      "Processing by BLIP: video_frames/frame$$8.0$$.jpg\n",
      "\n",
      "FRAME-BY-FRAME DESCRIPTION:\n",
      " time code 0.0: a man in a suit and tie holding a piece of paper.\n",
      "time code 1.0: a man in a suit and tie holding a piece of paper.\n",
      "time code 2.0: a man in a suit and tie holding a piece of paper.\n",
      "time code 3.0: a man in a suit and tie standing in front of a screen.\n",
      "time code 4.0: a man in a suit and tie is looking at something.\n",
      "time code 5.0: a man in a suit and tie is looking at something.\n",
      "time code 6.0: a man in a suit and tie looking at the camera.\n",
      "time code 7.0: a man in a suit and tie standing in front of a window\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"image-to-text\", model=\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "video = cv2.VideoCapture(\"tony.mp4\")\n",
    "\n",
    "# Set the desired frequency (in seconds)\n",
    "save_frequency = 1\n",
    "\n",
    "# Loop through all the frames\n",
    "is_frame_available, frame = video.read()\n",
    "frame_count = 0\n",
    "\n",
    "while is_frame_available:\n",
    "    # Get the current timestamp of the frame\n",
    "    timestamp = video.get(cv2.CAP_PROP_POS_MSEC) / 1000  # Convert from milliseconds to seconds\n",
    "    \n",
    "    # Save the frame as an image file with the timestamp in the filename\n",
    "    if frame_count % int(save_frequency * video.get(cv2.CAP_PROP_FPS)) == 0:\n",
    "        cv2.imwrite(f'video_frames/frame$${timestamp:.1f}$$.jpg', frame)\n",
    "        print(f\"Video processing: {timestamp:.1f}\")\n",
    "    \n",
    "    \n",
    "    # Read the next frame\n",
    "    is_frame_available, frame = video.read()\n",
    "    frame_count += 1\n",
    "\n",
    "    # Add a delay to regulate the frequency\n",
    "    time.sleep(1 / video.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "# Release the video file\n",
    "video.release()\n",
    "\n",
    "image_path = os.listdir(\"video_frames\")\n",
    "sorted_images = sorted(image_path, key=lambda x: float(x.split('$$')[1]))\n",
    "\n",
    "descriptions = []\n",
    "for idx, image in enumerate(sorted_images):\n",
    "    print(\"Processing by BLIP: video_frames/\" + image)\n",
    "    descriptions.append(\"time code \" + sorted_images[idx].split(\"$$\")[1] + \": \" + pipe(\"video_frames/\" + image)[0].get(\"generated_text\"))\n",
    "\n",
    "\n",
    "descriptions = descriptions[:-1]\n",
    "joined_descriptions = \".\\n\".join(descriptions)\n",
    "prompt_descriptions = \". \".join(descriptions)\n",
    "\n",
    "print(\"\"\"\n",
    "FRAME-BY-FRAME DESCRIPTION:\\n\"\"\", joined_descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WHISPER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"automatic-speech-recognition\", model=\"AlanRobotics/whisper-tiny-ru\", tokenizer=WhisperTokenizer.from_pretrained(\"openai/whisper-tiny\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in tony.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "from moviepy.editor import VideoFileClip\n",
    "\n",
    "# Load the video file\n",
    "video = VideoFileClip('/Users/a.gazzaev/Desktop/dl_lab3/LLM_and_BLIP/tony.mp4')\n",
    "\n",
    "# Extract the audio\n",
    "audio = video.audio\n",
    "\n",
    "# Save the audio file\n",
    "audio.write_audiofile('tony.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Я железный человек.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"tony.wav\", 'rb') as f:\n",
    "    audio = f.read()\n",
    "\n",
    "transcription = pipe(audio).get(\"text\")\n",
    "transcription"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VICUNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(prompt):\n",
    "    res = requests.get(url=\"http://10.207.0.31:5005/get_answer\", json={\"prompt\": f\"{prompt}\"})\n",
    "    answer = res.json()[\"result\"][2:]\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MODEL RESPONSE:  На видео показан один человек, надетый в костюм и галстук. В первых трех кадрах он держит лист бумаги. На четвертом кадре он стоят перед экраном. На пятом и шестом кадрах он смотрит на что-то. На седьмом кадре он смотрит в камеру. На протяжении всего видео он произносит фразу \"Я железный человек\".\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"This is a text description of consecutive frames with timecodes from the video. The video shows only one person. Based on these descriptions, try to briefly tell what is happening on the video? This video also have audio from person on video: {transcription}. Что происходит на видео? Ответь на русском\"\n",
    "res = get_response(prompt=(prompt + prompt_descriptions))\n",
    "print(\"\"\"\n",
    "MODEL RESPONSE: \"\"\", res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
